{"class_ref":{"module":"saeco.architectures.matryoshka_clt","cls_name":"MatryoshkaCLT","source_backup":"import random\nfrom functools import cached_property\n\nimport einops\nimport nnsight\nimport saeco.components.features.features as ft\nimport saeco.core as cl\nimport torch\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom saeco.architecture import Architecture, aux_model_prop, loss_prop, model_prop, SAE\n\nfrom saeco.components import L2Loss, Lambda, Loss, SparsityPenaltyLoss\nfrom saeco.components.features.features_param import FeaturesParam\n\nfrom saeco.components.jumprelu import JumpReLU\nfrom saeco.components.ops import Indexer\nfrom saeco.components.sae_cache import SAECache\nfrom saeco.core import Seq\nfrom saeco.core.reused_forward import ReuseForward\n\nfrom saeco.data.model_cfg import ActsDataConfig\nfrom saeco.initializer.initializer import Initializer\nfrom saeco.misc import useif\nfrom saeco.misc.nnsite import getsite, setsite\nfrom saeco.sweeps.sweepable_config.sweepable_config import SweepableConfig\nfrom saeco.trainer.evaluation_protocol import ReconstructionEvaluatorFunctionProtocol\nfrom saeco.trainer.recons import to_losses\nfrom saeco.trainer.trainable import Trainable\n\n\nclass MatryoshkaCLTConfig(SweepableConfig):\n    n_sites: int = 12\n    n_nestings: int = 3\n\n    per_decoder_split: bool = True\n\n\ndef reconstruct_clt_layer(sae: Trainable, mlp_inputs: list[torch.Tensor], num_layers):\n    cache = SAECache()\n    tensors = mlp_inputs + [\n        torch.zeros_like(mlp_inputs[0]) for _ in range(num_layers - len(mlp_inputs))\n    ]\n\n    catted = torch.cat(tensors, dim=-1).float()\n\n    results = sae(catted[0], cache=cache).to(mlp_inputs[0].dtype)[-1, ...].unsqueeze(0)\n    result = results.chunk(num_layers, dim=-1)[len(mlp_inputs) - 1]\n\n    return result\n\n\ndef with_sae_runner(\n    model: nnsight.LanguageModel, encoder: Trainable, cfg: ActsDataConfig\n):\n\n    def saerunner(tokens):\n        mlp_inputs = []\n\n        with model.trace(tokens):\n            for i in range(12):\n                mlp_input_site = f\"transformer.h.{i}.mlp.input\"\n                mlp_output_site = f\"transformer.h.{i}.mlp.output\"\n\n                mlp_input = getsite(model, mlp_input_site).save()\n\n                mlp_inputs.append(mlp_input)\n\n                acts_re = nnsight.apply(reconstruct_clt_layer, encoder, mlp_inputs, 12)\n\n                setsite(model, mlp_output_site, acts_re)\n\n            out = model.output.logits.save()\n\n        return out\n\n    return saerunner\n\n\ndef zero_ablated_runner(model: nnsight.LanguageModel, cfg: ActsDataConfig):\n    def zrunner(tokens):\n        with model.trace(tokens) as tracer:\n            for site in cfg.sites:\n                if \"input\" in site:\n                    lm_acts = getsite(model, site)\n                    acts_re = nnsight.apply(torch.zeros_like, lm_acts)\n                    patch_in = acts_re\n                    setsite(model, site, patch_in)\n\n            out = model.output.logits.save()\n        return out\n\n    return zrunner\n\n\ndef normal_runner(model: nnsight.LanguageModel, cfg: ActsDataConfig):\n    def nrunner(tokens):\n        with model.trace(tokens):\n            out = model.output.logits.save()\n        return out\n\n    return nrunner\n\n\n@torch.inference_mode()\ndef get_multisite_recons_loss(\n    llm,\n    sae: Trainable,\n    tokens=None,\n    num_batches=10,\n    cfg: ActsDataConfig = None,\n    batch_size=1,\n    cast_fn=...,\n):\n    cfg = cfg or sae.cfg\n    loss_list = []\n\n    with_sae = to_losses(with_sae_runner(llm, sae, cfg))\n    zero = to_losses(zero_ablated_runner(llm, cfg))\n    normal = to_losses(normal_runner(llm, cfg))\n    rand_tokens = tokens[torch.randperm(len(tokens))]\n    with cast_fn():\n        for i in range(num_batches):\n            batch_tokens = rand_tokens[i * batch_size : (i + 1) * batch_size].cuda()\n            zeroed = zero(batch_tokens)\n            re = with_sae(batch_tokens)\n            loss = normal(batch_tokens)\n            loss_list.append((loss, re, zeroed))\n    losses = torch.tensor(loss_list)\n    loss, recons_loss, zero_abl_loss = losses.mean(0).tolist()\n    print(loss, recons_loss, zero_abl_loss)\n    score = (zero_abl_loss - recons_loss) / (zero_abl_loss - loss)\n    print(f\"{score:.2%}\")\n    return {\n        \"recons_score\": score,\n        \"nats_lost\": recons_loss - loss,\n        \"loss\": loss,\n        \"recons_loss\": recons_loss,\n        \"zero_ablation_loss\": zero_abl_loss,\n    }\n\n\nclass MatryoshkaLoss(Loss):\n    def loss(self, x, y, y_pred, cache: SAECache):\n        return torch.mean((y.unsqueeze(0) - y_pred) ** 2)\n\n\ndef slice_dim(low, high, dim):\n    return (slice(None),) * dim + (slice(low, high),)\n\n\ndef split_tensor(x, bounds, dim):\n    if dim == -1:\n        dim = x.dim()\n\n    if bounds[0] != 0:\n        bounds = [0] + bounds\n    if bounds[-1] != x.shape[dim]:\n        bounds = bounds + [x.shape[dim]]\n\n    return [x[slice_dim(bounds[i], bounds[i + 1], dim)] for i in range(len(bounds) - 1)]\n\n\ndef generate_random_boundary(n_slices, max_index):\n    return sorted(\n        [0]\n        + [random.randint(1, max_index - 1) for _ in range(n_slices - 1)]\n        + [max_index]\n    )\n\n\nclass SplittableDecoder(\n    cl.Module, ft.OrthogonalizeFeatureGradsMixin, ft.NormFeaturesMixin\n):\n    def __init__(\n        self, d_dict, num_layers, d_data, num_nestings, per_decoder_split=False\n    ):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(num_layers * d_dict, d_data))\n\n        self.d_dict = d_dict\n        self.n_nestings = num_nestings\n        self.n_layers = num_layers\n        self.d_data = d_data\n        self.per_decoder_split = per_decoder_split\n\n    def forward(self, input, *, cache: SAECache):\n        # If a boundary is shared by all decoders, then the input is a tuple of (acts, splits)\n        if self.per_decoder_split:\n            splits = generate_random_boundary(self.n_nestings, self.d_dict)\n            acts = input\n        else:\n            acts, splits = input\n\n        return torch.stack(\n            [\n                torch.einsum(\"lbd,ldo->bo\", x_i, dec_i)\n                for (dec_i, x_i) in zip(\n                    self.split_dec(splits), split_tensor(acts, splits, dim=2)\n                )\n            ],\n            dim=0,\n        ).cumsum(dim=0)\n\n    def split_dec(self, bounds: list[int]):\n        return split_tensor(\n            einops.rearrange(\n                self.weight, \"(l d_dict) d_data -> l d_dict d_data\", l=self.n_layers\n            ),\n            bounds,\n            dim=1,\n        )\n\n    @cached_property\n    def features(self):\n        return {\n            \"weight\": FeaturesParam(\n                self.weight,\n                feature_index=0,\n                feature_parameter_type=FeaturesParam.FPTYPES.dec,\n                param_id=f\"{self.n_layers - 1}\",\n            ),\n        }\n\n\nclass AddSplitDecoderBias(cl.Module):\n    def __init__(self, num_layers, d_layer_data):\n        super().__init__()\n        self.bias = nn.Parameter(torch.randn(num_layers * d_layer_data)).cuda()\n\n    def forward(self, x, *, cache: SAECache):\n        return x + self.bias\n\n\nclass MatryoshkaCLTDecoder(cl.Module):\n    def __init__(\n        self,\n        d_layer_data: int,\n        d_layer_dict: int,\n        cfg: MatryoshkaCLTConfig,\n    ):\n        super().__init__()\n\n        self.d_layer_data = d_layer_data\n        self.d_layer_dict = d_layer_dict\n\n        self.bias = AddSplitDecoderBias(\n            num_layers=cfg.n_sites, d_layer_data=d_layer_data\n        )\n\n        self.decoders = [\n            SplittableDecoder(\n                d_dict=d_layer_dict,\n                num_layers=n + 1,\n                d_data=d_layer_data,\n                num_nestings=cfg.n_nestings,\n                per_decoder_split=cfg.per_decoder_split,\n            )\n            for n in range(cfg.n_sites)\n        ]\n\n        self.weights = [dec.weight for dec in self.decoders]\n\n        self.decode = Seq(\n            split_into_layers=Lambda(\n                lambda x: einops.rearrange(\n                    x, \"batch (layer d_dict) -> layer batch d_dict\", d_dict=d_layer_dict\n                )\n            ),\n            route_to_decoders=cl.Parallel(\n                *[Indexer.L[: i + 1] for i in range(cfg.n_sites)]\n            ).reduce(lambda *x: x),\n            **useif(\n                not cfg.per_decoder_split,\n                generate_boundary=Seq(\n                    generate_splits=Lambda(\n                        lambda x: (\n                            x,\n                            generate_random_boundary(cfg.n_nestings, d_layer_dict),\n                        )\n                    ),\n                    prepare_splits=Lambda(lambda x: [(x_i, x[1]) for x_i in x[0]]),\n                ),\n            ),\n            splittable_decoders=cl.Router(\n                *[\n                    SplittableDecoder(\n                        d_dict=d_layer_dict,\n                        num_layers=n + 1,\n                        d_data=d_layer_data,\n                        num_nestings=cfg.n_nestings,\n                        per_decoder_split=cfg.per_decoder_split,\n                    )\n                    for n in range(cfg.n_sites)\n                ]\n            ).reduce(lambda *x: torch.cat(x, dim=-1)),\n            decoder_bias=AddSplitDecoderBias(\n                num_layers=cfg.n_sites, d_layer_data=d_layer_data\n            ),\n        )\n\n    def forward(self, x, *, cache: SAECache):\n        return cache(self).decode(x)\n\n\nclass MatryoshkaCLT(Architecture[MatryoshkaCLTConfig]):\n    def boundary_generator(self):\n        return [self.d_layer_dict // (2**i) for i in range(self.cfg.n_nestings)]\n\n    @cached_property\n    def decoder(self):\n        return MatryoshkaCLTDecoder(self.d_layer_data, self.d_layer_dict, self.cfg)\n\n    @cached_property\n    def initializers(self):\n        return self.init.split_initializer(self.cfg.n_sites)\n\n    @cached_property\n    def pre_encoders(self):\n        return ReuseForward(\n            Seq(\n                split=Lambda(lambda x: torch.chunk(x, self.cfg.n_sites, dim=-1)),\n                encode=cl.Router(\n                    *[self.initializers[i].encoder for i in range(self.cfg.n_sites)]\n                ).reduce(lambda *x: torch.cat(x, dim=-1)),\n            )\n        )\n\n    def setup(self):\n        assert self.init.d_dict % self.cfg.n_sites == 0\n        assert self.init.d_data % self.cfg.n_sites == 0\n        self.d_layer_dict = self.init.d_dict // self.cfg.n_sites\n        self.d_layer_data = self.init.d_data // self.cfg.n_sites\n\n        assert (2 ** (self.cfg.n_nestings - 1)) <= self.d_layer_dict\n\n        self.nesting_boundaries = self.boundary_generator()\n\n    @model_prop\n    def model(self):\n        return SAE(\n            encoder_pre=self.pre_encoders,\n            #            nonlinearity=JumpReLU(0.03, 1),\n            nonlinearity=nn.ReLU(),\n            decoder=self.decoder,\n        )\n\n    @loss_prop\n    def l2_loss(self):\n        return MatryoshkaLoss(self.model)\n\n    @loss_prop\n    def sparsity_loss(self):\n        return SparsityPenaltyLoss(self.model)\n\n    def get_evaluation_functions(\n        self,\n    ) -> dict[str, ReconstructionEvaluatorFunctionProtocol]:\n        return {\"recons/\": get_multisite_recons_loss}\n"},"config":{"train_cfg":{"data_cfg":{"dataset":"alancooney/sae-monology-pile-uncopyrighted-tokenizer-gpt2","load_from_disk":false,"model_cfg":{"model_name":"gpt2","acts_cfg":{"d_data":768,"sites":["transformer.h.0.mlp.input","transformer.h.1.mlp.input","transformer.h.2.mlp.input","transformer.h.3.mlp.input","transformer.h.4.mlp.input","transformer.h.5.mlp.input","transformer.h.6.mlp.input","transformer.h.7.mlp.input","transformer.h.8.mlp.input","transformer.h.9.mlp.input","transformer.h.10.mlp.input","transformer.h.11.mlp.input","transformer.h.0.mlp.output","transformer.h.1.mlp.output","transformer.h.2.mlp.output","transformer.h.3.mlp.output","transformer.h.4.mlp.output","transformer.h.5.mlp.output","transformer.h.6.mlp.output","transformer.h.7.mlp.output","transformer.h.8.mlp.output","transformer.h.9.mlp.output","transformer.h.10.mlp.output","transformer.h.11.mlp.output"],"site_d_datas":null,"excl_first":true,"filter_pad":true,"storage_dtype_str":"bfloat16","autocast_dtype_str":"bfloat16","force_cast_dtype_str":"bfloat16"},"model_kwargs":{},"torch_dtype_str":null},"trainsplit":{"split":"train","start":0,"end":5,"act_chunks_cached":2,"acts_per_chunk":100000},"testsplit":{"split":"train","start":80,"end":90,"act_chunks_cached":null,"acts_per_chunk":null},"valsplit":{"split":"train","start":90,"end":100,"act_chunks_cached":null,"acts_per_chunk":null},"set_bos":true,"seq_len":256,"tokens_column_name":"input_ids","generation_config":{"tokens_per_pile":67108864,"acts_per_pile":32768,"meta_batch_size":65536,"llm_batch_size":32768,"num_document_distribution_batches":100,"compress_acts":true},"perm_all":false,"databuffer_num_workers":4,"databuffer_queue_size":32,"databuffer_worker_queue_base_size":1,"databuffer_worker_offset_mult":2},"wandb_cfg":{"project":"sae sweeps"},"coeffs":{"sparsity_loss":0.0000931067009299635,"L2_loss":1.0},"l0_targeter_type":"gentle_basic","l0_target":600.0,"l0_targeting_enabled":true,"l0_target_adjustment_size":0.001,"use_autocast":true,"batch_size":32,"lr":0.001,"betas":[0.9,0.997],"use_lars":false,"kwargs":{},"optim":"Adam","raw_schedule_cfg":{"run_length":10000,"resample_period":100000,"resample_delay":0,"resampling_finished_phase":0.3,"targeting_post_resample_step_size_warmup":0.2,"targeting_post_resample_hiatus":0.2,"targeting_delay":0,"targeting_warmup_length":0.15,"targeting_pre_deflation":null,"lr_warmup_length":500,"lr_end_plateau_length":0,"lr_cooldown_length":0.2,"lr_resample_warmup_length":0.2,"lr_warmup_factor":0.1,"lr_cooldown_factor":0.1,"lr_resample_warmup_factor":0.1,"lr_geometric_rescale":true},"use_averaged_model":false,"checkpoint_period":null,"save_on_complete":true,"weight_decay":null,"intermittent_metric_freq":100000,"input_sites":["transformer.h.0.mlp.input","transformer.h.1.mlp.input","transformer.h.2.mlp.input","transformer.h.3.mlp.input","transformer.h.4.mlp.input","transformer.h.5.mlp.input","transformer.h.6.mlp.input","transformer.h.7.mlp.input","transformer.h.8.mlp.input","transformer.h.9.mlp.input","transformer.h.10.mlp.input","transformer.h.11.mlp.input"],"target_sites":["transformer.h.0.mlp.output","transformer.h.1.mlp.output","transformer.h.2.mlp.output","transformer.h.3.mlp.output","transformer.h.4.mlp.output","transformer.h.5.mlp.output","transformer.h.6.mlp.output","transformer.h.7.mlp.output","transformer.h.8.mlp.output","transformer.h.9.mlp.output","transformer.h.10.mlp.output","transformer.h.11.mlp.output"]},"arch_cfg":{"n_sites":12,"n_nestings":3,"per_decoder_split":true},"normalizer_cfg":{"mu_s":1,"mu_e":0,"std_s":1,"std_e":1,"sandwich":false},"resampler_config":{"optim_reset_cfg":{"optim_momentum":0.0,"bias_momentum":0.0,"dec_momentum":false,"b2_technique":"sq","b2_scale":1.0},"bias_reset_value":0.0,"dead_threshold":3e-6,"freq_balance":null,"freq_balance_strat":"sep","expected_biases":2,"expected_decs":1,"expected_encs":1,"enc_directions":0,"dec_directions":1},"init_cfg":{"d_data":9216,"dict_mult":8}}}